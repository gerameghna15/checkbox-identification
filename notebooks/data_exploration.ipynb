{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7339aa06",
   "metadata": {},
   "source": [
    "# Dataset Exploration – Checkbox Identification\n",
    "\n",
    "This notebook presents a **retrospective exploration** of the datasets used in this\n",
    "project for checkbox state classification (checked vs unchecked) using a\n",
    "vision-language model.\n",
    "\n",
    "The original dataset was sourced from Roboflow Universe in COCO format and was\n",
    "designed for checkbox **object detection**. As part of the experimental workflow,\n",
    "bounding box annotations were used to crop checkbox regions and construct\n",
    "classification-ready datasets. A reduced and balanced subset was later created\n",
    "for efficient model training and evaluation.\n",
    "\n",
    "The purpose of this notebook is to document and analyze the **final dataset\n",
    "pipeline**, including:\n",
    "- the original COCO dataset structure,\n",
    "- the cropped checkbox dataset,\n",
    "- and the reduced dataset used for training.\n",
    "\n",
    "The goals of this analysis are to:\n",
    "- Understand dataset structure at each stage of processing,\n",
    "- Inspect class distribution across train, validation, and test splits,\n",
    "- Identify dataset challenges such as class imbalance and visual ambiguity in\n",
    "  checkbox regions.\n",
    "\n",
    "This exploration provides the foundation for the preprocessing choices and model\n",
    "design decisions applied in the subsequent stages of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6214c23",
   "metadata": {},
   "source": [
    "## Original Dataset Source\n",
    "\n",
    "The primary dataset used in this project was sourced from **Roboflow Universe**, specifically\n",
    "the *Checkbox Detection* dataset (Version 7):\n",
    "\n",
    "https://universe.roboflow.com/checkbox-detection-ztyeq/checkbox-kwtcz-qkbid/dataset/7/images\n",
    "\n",
    "This dataset contains 843 document images annotated for checkbox object detection, with\n",
    "predefined train, validation, and test splits. The images include a variety of real-world\n",
    "document types such as scanned forms, UI screenshots, and application interfaces, making\n",
    "the dataset diverse and representative.\n",
    "\n",
    "While the original dataset is designed for object detection, it was selected for this\n",
    "project due to the quality and diversity of checkbox annotations. These annotations were\n",
    "used to extract checkbox regions, enabling the construction of a downstream checkbox\n",
    "state classification dataset (checked vs unchecked).\n",
    "\n",
    "The dataset was downloaded **COCO format**\n",
    "(`checkbox.v7i.coco`). The COCO annotation format provides bounding box\n",
    "coordinates and class labels for object detection tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583271d",
   "metadata": {},
   "source": [
    "## Motivation for Cropping Checkbox Regions\n",
    "\n",
    "To adapt the detection dataset for checkbox state classification, the annotated\n",
    "bounding boxes were used to crop checkbox regions from the original images.\n",
    "\n",
    "Cropping serves several purposes:\n",
    "- Removes irrelevant background content\n",
    "- Focuses the model on local visual cues\n",
    "- Reduces input complexity\n",
    "- Enables consistent input size for training\n",
    "\n",
    "This transformation converts the detection dataset into a classification-ready\n",
    "dataset while preserving the original annotation information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8785d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO dataset exists: True\n",
      "Splits: ['valid', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Path to COCO dataset root\n",
    "COCO_ROOT = Path(\"../data/raw/checkbox.v7i.coco\")\n",
    "\n",
    "print(\"COCO dataset exists:\", COCO_ROOT.exists())\n",
    "print(\"Splits:\", [p.name for p in COCO_ROOT.iterdir() if p.is_dir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9744fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID SET — Images: 151\n",
      "TEST SET — Images: 81\n",
      "TRAIN SET — Images: 611\n",
      "\n",
      "TOTAL IMAGES IN ORIGINAL COCO DATASET: 843\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "COCO_ROOT = Path(\"../data/raw/checkbox.v7i.coco\")\n",
    "\n",
    "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "def count_images_robust(dataset_root):\n",
    "    counts = {}\n",
    "    for split in dataset_root.iterdir():\n",
    "        if split.is_dir():\n",
    "            images = [\n",
    "                p for p in split.rglob(\"*\")\n",
    "                if p.suffix.lower() in IMAGE_EXTENSIONS\n",
    "            ]\n",
    "            counts[split.name] = len(images)\n",
    "    return counts\n",
    "\n",
    "coco_image_counts = count_images_robust(COCO_ROOT)\n",
    "coco_image_counts\n",
    "\n",
    "total = 0\n",
    "for split, count in coco_image_counts.items():\n",
    "    print(f\"{split.upper()} SET — Images: {count}\")\n",
    "    total += count\n",
    "\n",
    "print(\"\\nTOTAL IMAGES IN ORIGINAL COCO DATASET:\", total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4eea9",
   "metadata": {},
   "source": [
    "## Cropped Checkbox Dataset\n",
    "\n",
    "Using the bounding box annotations from the original COCO dataset, checkbox regions\n",
    "were cropped from the full document images. The resulting dataset was stored in the\n",
    "folder `cropped_checkboxes_binary`.\n",
    "\n",
    "This dataset represents the first classification-ready version of the data, where\n",
    "each sample corresponds to a single checkbox labeled as either **checked** or\n",
    "**unchecked**. The cropped dataset retains the original train, validation, and test\n",
    "splits to avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be56fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALID SET — Total cropped checkboxes: 978\n",
      "  unchecked: 451 (46.11%)\n",
      "  checked: 527 (53.89%)\n",
      "\n",
      "TEST SET — Total cropped checkboxes: 714\n",
      "  unchecked: 481 (67.37%)\n",
      "  checked: 233 (32.63%)\n",
      "\n",
      "TRAIN SET — Total cropped checkboxes: 4403\n",
      "  unchecked: 2374 (53.92%)\n",
      "  checked: 2029 (46.08%)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CROPPED_FULL = Path(\"../data/processed/cropped_checkboxes_binary\")\n",
    "\n",
    "def explore_cropped_dataset(dataset_root):\n",
    "    stats = {}\n",
    "    for split in dataset_root.iterdir():\n",
    "        if split.is_dir():\n",
    "            split_stats = {}\n",
    "            for cls in split.iterdir():\n",
    "                if cls.is_dir():\n",
    "                    split_stats[cls.name] = len(list(cls.glob(\"*\")))\n",
    "            stats[split.name] = split_stats\n",
    "    return stats\n",
    "\n",
    "cropped_full_stats = explore_cropped_dataset(CROPPED_FULL)\n",
    "cropped_full_stats\n",
    "\n",
    "for split, classes in cropped_full_stats.items():\n",
    "    total = sum(classes.values())\n",
    "    print(f\"\\n{split.upper()} SET — Total cropped checkboxes: {total}\")\n",
    "    for cls, count in classes.items():\n",
    "        pct = (count / total) * 100 if total > 0 else 0\n",
    "        print(f\"  {cls}: {count} ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a13406",
   "metadata": {},
   "source": [
    "## Observations on Cropped Dataset (`cropped_checkboxes_binary`)\n",
    "\n",
    "The cropped dataset contains a total of 6,095 checkbox samples distributed across\n",
    "training, validation, and test splits. Each sample is labeled as either **checked**\n",
    "or **unchecked**.\n",
    "\n",
    "Analysis of the class distribution reveals moderate class imbalance across splits.\n",
    "While the training and validation sets show relatively balanced proportions, the\n",
    "test split exhibits a noticeable skew toward unchecked samples.\n",
    "\n",
    "Such imbalance can bias model evaluation and make performance metrics less reliable.\n",
    "Additionally, the full cropped dataset is relatively large, increasing computational\n",
    "cost during fine-tuning experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb77c6b",
   "metadata": {},
   "source": [
    "## Motivation for Dataset Reduction\n",
    "\n",
    "To address class imbalance and reduce computational overhead, a smaller and balanced\n",
    "subset of the cropped dataset was constructed. This reduced dataset preserves the\n",
    "original train, validation, and test splits while ensuring equal representation of\n",
    "checked and unchecked samples.\n",
    "\n",
    "The resulting dataset, stored as `cropped_checkboxes_binary_small`, was used for all\n",
    "model training and evaluation experiments in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c51f4198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALID SET — Final images used: 300\n",
      "  unchecked: 150 (50.00%)\n",
      "  checked: 150 (50.00%)\n",
      "\n",
      "TEST SET — Final images used: 400\n",
      "  unchecked: 200 (50.00%)\n",
      "  checked: 200 (50.00%)\n",
      "\n",
      "TRAIN SET — Final images used: 1000\n",
      "  unchecked: 500 (50.00%)\n",
      "  checked: 500 (50.00%)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CROPPED_SMALL = Path(\"../data/processed/cropped_checkboxes_binary_small\")\n",
    "\n",
    "def explore_small_dataset(dataset_root):\n",
    "    stats = {}\n",
    "    for split in dataset_root.iterdir():\n",
    "        if split.is_dir():\n",
    "            split_stats = {}\n",
    "            for cls in split.iterdir():\n",
    "                if cls.is_dir():\n",
    "                    split_stats[cls.name] = len(list(cls.glob(\"*\")))\n",
    "            stats[split.name] = split_stats\n",
    "    return stats\n",
    "\n",
    "cropped_small_stats = explore_small_dataset(CROPPED_SMALL)\n",
    "cropped_small_stats\n",
    "\n",
    "for split, classes in cropped_small_stats.items():\n",
    "    total = sum(classes.values())\n",
    "    print(f\"\\n{split.upper()} SET — Final images used: {total}\")\n",
    "    for cls, count in classes.items():\n",
    "        pct = (count / total) * 100 if total > 0 else 0\n",
    "        print(f\"  {cls}: {count} ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7c251",
   "metadata": {},
   "source": [
    "## Reduced and Balanced Dataset (`cropped_checkboxes_binary_small`)\n",
    "\n",
    "Based on the analysis of the full cropped dataset, a reduced and balanced subset was\n",
    "constructed to support efficient and unbiased model training. The reduced dataset\n",
    "ensures equal representation of checked and unchecked samples across all splits.\n",
    "\n",
    "The final dataset used for training consists of:\n",
    "- **1000** training images\n",
    "- **300** validation images\n",
    "- **400** test images\n",
    "\n",
    "Each split maintains a 50–50 class balance, which helps prevent bias during training\n",
    "and enables reliable evaluation of model performance. This dataset was used for all\n",
    "baseline and fine-tuning experiments reported in this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf06e79",
   "metadata": {},
   "source": [
    "## Dataset Summary\n",
    "\n",
    "| Dataset Stage        | Train | Validation | Test |\n",
    "|----------------------|-------|------------|------|\n",
    "| Original (COCO)      | 611   | 151        | 81   |\n",
    "| Cropped (Full)       | 4403  | 978        | 714  |\n",
    "| Cropped (Small)      | 1000  | 300        | 400  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
